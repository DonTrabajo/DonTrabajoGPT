# DonTrabajoGPT Orchestrator Design
**Version:** 1.0
**Date:** 2025-11-23
**Author:** Claude (Architecture & Design Brain)
**Status:** Ready for Implementation

---

## 1. Orchestrator Overview

The **orchestrator** is the central coordinator and workflow engine for DonTrabajoGPT. It sits between the CLI/TUI presentation layer and the underlying analysis modules (linPEAS pipeline, CVE matching, LLM backends, agent mode). The orchestrator owns **workflow logic, LLM routing decisions, and data pipeline coordination**â€”transforming user intent (e.g., "analyze this linPEAS output with local LLM") into a series of module calls while managing temporary state, error handling, and result aggregation.

The orchestrator explicitly does **NOT** own: UI/presentation logic (no Rich console calls), raw file I/O operations (delegates to existing modules), or module-specific business logic (linPEAS regex extraction, CVE version matching, etc.). It consumes clean APIs from `linpeas_preprocessor`, `cve_matcher`, `gpt_analysis`, and `oss_client`, returning structured data or summary strings back to the caller. This separation allows the TUI to remain purely presentational while the orchestrator becomes the single source of truth for multi-step analysis workflows.

---

## 2. API Design for `orchestrator.py`

### 2.1 Core Analysis Workflows

#### `analyze_linpeas(raw_file_path: str, mode: str = "auto", save_json: bool = False) -> dict`

Run the full linPEAS analysis pipeline: preprocess â†’ parse â†’ CVE match â†’ optional LLM summary.

**Parameters:**
- `raw_file_path` (str): Path to raw linPEAS .txt output
- `mode` (str): LLM summarization mode. One of:
  - `"none"` â€” Skip LLM analysis entirely
  - `"local"` â€” Use Ollama/local LLM via `oss_client`
  - `"cloud"` â€” Use OpenAI/cloud LLM via `gpt_analysis`
  - `"auto"` â€” Automatic routing (see Â§ 3)
- `save_json` (bool): If True, preserve the intermediate JSON file instead of deleting it

**Returns:**
```python
{
    "status": "success" | "partial" | "error",
    "json_path": str | None,  # path to intermediate JSON if save_json=True
    "parsed_data": dict,       # structured linPEAS data
    "cve_findings": list,      # CVE matches from cve_matcher
    "llm_summary": str | None, # LLM-generated summary (if mode != "none")
    "errors": list             # any non-fatal errors encountered
}
```

**Example:**
```python
result = orchestrator.analyze_linpeas(
    "linpeas_output.txt",
    mode="local",
    save_json=False
)
if result["status"] == "success":
    print(result["llm_summary"])
```

---

#### `preprocess_only(raw_file_path: str, output_path: str | None = None) -> dict`

Run only the preprocessing step, returning path to generated JSON.

**Parameters:**
- `raw_file_path` (str): Path to raw linPEAS .txt output
- `output_path` (str | None): Custom output path, or auto-generate timestamp-based name

**Returns:**
```python
{
    "status": "success" | "error",
    "json_path": str | None,
    "parsed_data": dict | None,
    "error": str | None
}
```

---

#### `run_cve_pipeline(json_path: str) -> dict`

Run CVE matcher against pre-existing linPEAS JSON.

**Parameters:**
- `json_path` (str): Path to linPEAS JSON (must match schema "don-trabajo-linpeas-v1")

**Returns:**
```python
{
    "status": "success" | "error",
    "cve_findings": list,  # CVE match results
    "error": str | None
}
```

**Example:**
```python
cve_result = orchestrator.run_cve_pipeline("linpeas_parsed_20251123.json")
for finding in cve_result["cve_findings"]:
    print(f"{finding['cve']}: {finding['description']}")
```

---

### 2.2 LLM Summarization

#### `summarize_findings(json_path: str | None = None, parsed_data: dict | None = None, mode: str = "auto") -> dict`

Generate LLM-powered summary of linPEAS findings. Accepts either a JSON file path OR parsed data dict.

**Parameters:**
- `json_path` (str | None): Path to linPEAS JSON file
- `parsed_data` (dict | None): Pre-loaded parsed data (mutually exclusive with json_path)
- `mode` (str): LLM backend selection ("local", "cloud", "auto")

**Returns:**
```python
{
    "status": "success" | "error",
    "summary": str | None,
    "backend_used": "local" | "cloud" | None,
    "error": str | None
}
```

**Example:**
```python
summary = orchestrator.summarize_findings(
    json_path="linpeas.json",
    mode="auto"
)
if summary["status"] == "success":
    print(f"Generated by: {summary['backend_used']}")
    print(summary["summary"])
```

---

### 2.3 Agent Mode

#### `launch_agent_session(persona: str = "don_trabajo", mode: str = "local") -> None`

Launch an interactive agent session (REPL) with specified persona.

**Parameters:**
- `persona` (str): Persona name (currently only "don_trabajo" supported)
- `mode` (str): Agent backend ("local" uses tools/agent/runner.py, future: "cloud")

**Returns:** None (blocking call until user exits session)

**Example:**
```python
orchestrator.launch_agent_session(persona="don_trabajo", mode="local")
# Drops into interactive REPL, returns when user types 'exit'
```

---

#### `agent_one_shot(query: str, persona: str = "don_trabajo", mode: str = "local") -> dict`

Run a single agent query without launching full REPL.

**Parameters:**
- `query` (str): User query/prompt
- `persona` (str): Persona to use
- `mode` (str): Backend selection

**Returns:**
```python
{
    "status": "success" | "error",
    "response": str | None,
    "error": str | None
}
```

---

### 2.4 LLM Routing & Availability

#### `detect_llm_backend(mode: str = "auto") -> str`

Determine which LLM backend to use based on availability and mode.

**Parameters:**
- `mode` (str): Requested mode ("local", "cloud", "auto")

**Returns:** str â€” Resolved backend: "local" | "cloud" | "none"

**Logic (see Â§ 3 for details):**
```python
if mode == "local":
    return "local" if _check_ollama_available() else "none"
elif mode == "cloud":
    return "cloud" if _check_openai_available() else "none"
elif mode == "auto":
    # Prefer local for OPSEC, fallback to cloud
    if _check_ollama_available():
        return "local"
    elif _check_openai_available():
        return "cloud"
    return "none"
```

---

#### `get_llm_status() -> dict`

Check availability of all LLM backends.

**Returns:**
```python
{
    "local": {
        "available": bool,
        "endpoint": str,  # e.g., "http://localhost:11434"
        "model": str      # e.g., "gpt-oss:20b"
    },
    "cloud": {
        "available": bool,
        "api_key_set": bool,
        "model": str      # e.g., "gpt-3.5-turbo"
    }
}
```

---

### 2.5 Utility Functions

#### `validate_linpeas_json(json_path: str) -> dict`

Validate that a JSON file matches the expected linPEAS schema.

**Returns:**
```python
{
    "valid": bool,
    "schema": str | None,
    "version": str | None,
    "missing_fields": list,
    "warnings": list
}
```

---

## 3. LLM Routing Rules

### 3.1 Mode Definitions

| Mode | Behavior |
|------|----------|
| `"local"` | **Force local LLM** (Ollama via `oss_client`). If unavailable, return error/skip summary. |
| `"cloud"` | **Force cloud LLM** (OpenAI via `gpt_analysis`). If unavailable, return error/skip summary. |
| `"auto"` | **Intelligent fallback**: Prefer local (OPSEC-safe) â†’ fallback to cloud if local unavailable â†’ skip if both unavailable. |
| `"none"` | **Skip LLM** analysis entirely (fast, offline-safe). |

### 3.2 Auto-Routing Logic

When `mode="auto"`, the orchestrator executes this decision tree:

```python
def detect_llm_backend(mode: str = "auto") -> str:
    """
    Determine which LLM backend to use.

    Priority for 'auto' mode:
    1. Local LLM (OPSEC-first, no external calls)
    2. Cloud LLM (fallback for convenience)
    3. None (graceful degradation)
    """
    if mode == "local":
        return "local" if _check_ollama_available() else "none"

    elif mode == "cloud":
        return "cloud" if _check_openai_available() else "none"

    elif mode == "auto":
        # Prefer local for operational security
        if _check_ollama_available():
            return "local"
        elif _check_openai_available():
            return "cloud"
        return "none"

    elif mode == "none":
        return "none"

    else:
        raise ValueError(f"Unknown mode: {mode}")
```

### 3.3 Availability Checks

#### Local LLM (Ollama)
```python
def _check_ollama_available() -> bool:
    """Check if Ollama is reachable at OLLAMA_HOST."""
    import os, requests
    host = os.getenv("OLLAMA_HOST", "http://localhost:11434")
    try:
        r = requests.get(f"{host}/api/tags", timeout=2)
        return r.status_code == 200
    except Exception:
        return False
```

#### Cloud LLM (OpenAI)
```python
def _check_openai_available() -> bool:
    """Check if OpenAI API key is configured."""
    import os
    from dotenv import load_dotenv
    load_dotenv()
    api_key = os.getenv("OPENAI_API_KEY")
    return bool(api_key and api_key.strip())
```

### 3.4 Routing Precedence (OPSEC-First)

**Rationale:** DonTrabajoGPT is an offensive security tool. Data exfiltration to cloud APIs is a liability during engagements. Auto mode prioritizes **local-first** processing:

1. **Local Ollama** â€” No external network calls, full control over model/data
2. **Cloud OpenAI** â€” Convenience fallback for prototyping/non-sensitive data
3. **None** â€” Graceful degradation (return raw parsed data without summary)

Users can override with explicit `mode="cloud"` when appropriate (e.g., CTF writeups, internal labs).

---

## 4. Menu Integration Plan

### 4.1 Current Menu â†’ Orchestrator Mapping

| Menu Option | Current Implementation | New Orchestrator Call |
|-------------|------------------------|----------------------|
| 0: Preprocess linPEAS | `linpeas_preprocessor.preprocess_linpeas_output()` | `orchestrator.preprocess_only(file_path, output_path)` |
| 1: Parse linPEAS | `linpeas_parser.parse_linpeas_output()` | Keep as-is (presentation-only, or wrap in orchestrator for consistency) |
| 2: CVE Matcher | `cve_matcher.run_cve_matcher()` | `orchestrator.run_cve_pipeline(json_path)` |
| 3: Tool Validation | `validate_tool_paths()` | Keep as-is (orthogonal to analysis pipeline) |
| 4: HTB Log Tracker | N/A (not implemented) | Future: `orchestrator.track_htb_session()` |
| 5: Discord Bot | N/A (not implemented) | Future: `orchestrator.launch_discord_bot()` |
| 6: Offline LLM Persona | `tools.oss_persona.tui_offline_llm.run()` | `orchestrator.launch_agent_session(mode="local")` |
| 7: Full linPEAS Analyzer | `combo_linpeas_analyzer.analyze_linpeas_full_stack()` | `orchestrator.analyze_linpeas(file_path, mode="auto", save_json=False)` |
| 8: Exit | Exit loop | Exit loop |

### 4.2 Updated `don_trabajo_gpt.py` Flow

**Current (tightly coupled):**
```python
elif choice == "7":
    file_path = _prompt_for_file("...")
    animated_transition()
    analyze_linpeas_full_stack(file_path)  # Direct call to combo module
```

**New (orchestrator-driven):**
```python
elif choice == "7":
    file_path = _prompt_for_file("...")
    animated_transition()

    # Orchestrator handles workflow
    result = orchestrator.analyze_linpeas(file_path, mode="auto", save_json=False)

    # TUI handles presentation
    if result["status"] in ["success", "partial"]:
        console.print(Panel("[green]âœ“ Analysis complete[/green]"))
        if result["llm_summary"]:
            console.print(Panel(result["llm_summary"], title="ðŸ§  AI Summary"))
    else:
        console.print(f"[red]âœ— Analysis failed: {result['errors']}[/red]")
```

### 4.3 Benefits of Orchestrator Pattern

1. **Single Source of Truth**: All workflow logic lives in `orchestrator.py`
2. **Testability**: Orchestrator functions return dictsâ€”easy to unit test without mocking Rich console
3. **Flexibility**: TUI can choose to display results differently (verbose, quiet, JSON export) without touching orchestrator
4. **Reusability**: CLI scripts, Discord bot, future web API can all call same orchestrator functions
5. **Maintainability**: Changes to pipeline order/LLM routing only touch orchestrator, not scattered across menu handlers

---

## 5. File Placement & Naming

### 5.1 Recommended Location

**Primary recommendation:** `orchestrator.py` in repo root

**Reasoning:**
- Sits at same level as existing pipeline modules (`linpeas_*.py`, `cve_matcher.py`, etc.)
- Consistent with current flat structure for core modules
- Easy imports: `from orchestrator import analyze_linpeas`

**Alternative (if repo grows):** `core/orchestrator.py`
- Better for future expansion (move all core modules to `core/`)
- Current repo size doesn't justify this yet

**Decision:** Use root-level `orchestrator.py` for now. Refactor to `core/` if module count exceeds 15-20 files.

### 5.2 Import Changes Required

#### In `don_trabajo_gpt.py`:
```python
# OLD
from combo_linpeas_analyzer import analyze_linpeas_full_stack
from linpeas_parser import parse_linpeas_output
from cve_matcher import run_cve_matcher
from linpeas_preprocessor import preprocess_linpeas_output

# NEW
import orchestrator
# Individual modules no longer imported directly for workflows
# (they become implementation details of orchestrator)
```

#### In `don_trabajo_gpt_tui.py`:
No changes requiredâ€”TUI remains purely presentational.

---

## 6. Implementation Notes for Codex

### Phase 1: Core Orchestrator Implementation

1. **Create `orchestrator.py` in repo root**
   - Start with module docstring explaining its role
   - Import dependencies: `os`, `json`, `datetime`, `requests`, `dotenv`
   - Import pipeline modules: `linpeas_preprocessor`, `linpeas_parser`, `linpeas_summarizer`, `cve_matcher`, `gpt_analysis`
   - Import LLM clients: `tools.oss_persona.oss_client` (for local LLM)

2. **Implement helper functions first:**
   - `_check_ollama_available() -> bool` â€” Ping Ollama at `$OLLAMA_HOST/api/tags` with 2s timeout
   - `_check_openai_available() -> bool` â€” Check if `OPENAI_API_KEY` env var is set
   - `_generate_timestamp_filename(prefix: str = "linpeas_parsed") -> str` â€” Return `prefix_YYYYMMDD-HHMMSS.json`

3. **Implement LLM routing:**
   - `detect_llm_backend(mode: str) -> str` â€” Use decision tree from Â§ 3.2
   - `get_llm_status() -> dict` â€” Return availability status for both backends

4. **Implement core analysis functions:**
   - `preprocess_only(raw_file_path, output_path) -> dict`
     - Call `linpeas_preprocessor.preprocess_linpeas_output()`
     - Suppress Rich console output (modules print directly; orchestrator returns data)
     - Return structured dict with status/json_path/error

   - `run_cve_pipeline(json_path) -> dict`
     - Load JSON, validate schema
     - Call `cve_matcher._match_cves()` directly (to avoid console printing)
     - Return findings as list

   - `summarize_findings(json_path, parsed_data, mode) -> dict`
     - Resolve backend via `detect_llm_backend(mode)`
     - If "local": call `oss_client.quick_answer()` with formatted prompt
     - If "cloud": call `gpt_analysis.format_prompt()` + `gpt_analysis._get_client()`
     - Return summary string + backend used

   - `analyze_linpeas(raw_file_path, mode, save_json) -> dict`
     - Chain: preprocess â†’ load JSON â†’ CVE match â†’ summarize (if mode != "none")
     - Aggregate results into single return dict
     - Handle cleanup of temp JSON if `save_json=False`
     - Catch all exceptions, return `status="error"` with error details

5. **Implement agent mode functions:**
   - `launch_agent_session(persona, mode) -> None`
     - Import `tools.agent.runner` dynamically
     - Call `runner.repl()` (blocking)

   - `agent_one_shot(query, persona, mode) -> dict`
     - Call `runner.chat(query)` programmatically
     - Return response as dict

6. **Add validation function:**
   - `validate_linpeas_json(json_path) -> dict`
     - Load JSON, check `metadata.schema == "don-trabajo-linpeas-v1"`
     - Check for required fields: `users`, `suid_binaries`, `kernel`, `binaries`
     - Return validation result

### Phase 2: Refactor Existing Modules

7. **Extract data-only functions from modules:**
   - In `cve_matcher.py`: Ensure `_match_cves(binaries)` is importable (already is)
   - In `gpt_analysis.py`: Ensure `format_prompt(parsed_data)` and `_get_client()` are importable (already are)
   - In `linpeas_parser.py`: Consider extracting `_validate_schema()` to orchestrator (or keep as-is)

8. **Update `combo_linpeas_analyzer.py`:**
   - Mark as deprecated (add docstring note)
   - Optionally rewrite as thin wrapper around orchestrator:
     ```python
     def analyze_linpeas_full_stack(raw_txt_path):
         # Legacy entry point - use orchestrator.analyze_linpeas() instead
         result = orchestrator.analyze_linpeas(raw_txt_path, mode="auto")
         # Print results using Rich (for backwards compatibility)
     ```

### Phase 3: Wire Orchestrator into TUI

9. **Update `don_trabajo_gpt.py` menu handlers:**
   - **Option 0** (Preprocess):
     ```python
     result = orchestrator.preprocess_only(file_path)
     if result["status"] == "success":
         console.print(f"[green]âœ“ JSON saved to: {result['json_path']}[/green]")
     ```

   - **Option 2** (CVE Matcher):
     ```python
     result = orchestrator.run_cve_pipeline(file_path)
     for finding in result.get("cve_findings", []):
         console.print(f"{finding['cve']}: {finding['description']}")
     ```

   - **Option 6** (Offline LLM):
     ```python
     orchestrator.launch_agent_session(persona="don_trabajo", mode="local")
     ```

   - **Option 7** (Full Analysis):
     ```python
     result = orchestrator.analyze_linpeas(file_path, mode="auto", save_json=False)
     # Display result["parsed_data"], result["cve_findings"], result["llm_summary"]
     ```

10. **Remove direct imports of pipeline modules:**
    - Delete: `from combo_linpeas_analyzer import analyze_linpeas_full_stack`
    - Delete: `from linpeas_preprocessor import preprocess_linpeas_output`
    - Keep: `from linpeas_parser import parse_linpeas_output` (Option 1 still uses it for display)
    - Keep: UI/transition imports (`animated_transition`, etc.)

### Phase 4: Testing & Validation

11. **Create test script `test_orchestrator.py`:**
    ```python
    import orchestrator

    # Test LLM detection
    status = orchestrator.get_llm_status()
    print(f"Local: {status['local']['available']}, Cloud: {status['cloud']['available']}")

    # Test preprocess
    result = orchestrator.preprocess_only("sample_linpeas.txt")
    assert result["status"] == "success"

    # Test full analysis
    result = orchestrator.analyze_linpeas("sample_linpeas.txt", mode="none")
    assert "parsed_data" in result
    ```

12. **Manual testing checklist:**
    - [ ] Run full analysis with `mode="local"` (Ollama running)
    - [ ] Run full analysis with `mode="cloud"` (API key set)
    - [ ] Run full analysis with `mode="auto"` (both available)
    - [ ] Run full analysis with `mode="auto"` (neither available) â†’ should return partial results
    - [ ] Launch agent session via menu option 6
    - [ ] Verify temp file cleanup when `save_json=False`
    - [ ] Verify JSON preservation when `save_json=True`

### Phase 5: Documentation

13. **Update `README.md`:**
    - Add section: "Architecture Overview"
    - Explain orchestrator role
    - Show example of calling orchestrator directly:
      ```python
      import orchestrator
      result = orchestrator.analyze_linpeas("linpeas.txt", mode="local")
      print(result["llm_summary"])
      ```

14. **Add docstrings to all orchestrator functions:**
    - Use Google-style docstrings
    - Include type hints for all parameters and returns
    - Add examples for complex functions

15. **Create `docs/api_reference.md`:**
    - Auto-generate from orchestrator docstrings (or write manually)
    - Document all public functions
    - Include return dict schemas

---

## 7. Future Enhancements

### 7.1 Short-term (Next Sprint)
- Add `export_results(result: dict, format: str = "json") -> str` â€” Export analysis to JSON/Markdown/HTML
- Add `batch_analyze(file_paths: list, mode: str) -> list` â€” Process multiple linPEAS outputs
- Implement `orchestrator.track_htb_session()` for menu option 4

### 7.2 Medium-term
- Add caching layer: `@lru_cache` for CVE lookups, avoid re-preprocessing same file
- Implement plugin system for custom analyzers
- Add `mode="hybrid"` â€” use local for parsing, cloud for final summary

### 7.3 Long-term
- REST API wrapper around orchestrator (FastAPI)
- Distributed analysis: submit jobs to remote workers
- Integration with MITRE ATT&CK framework for technique mapping

---

## 8. Design Principles

1. **Separation of Concerns**: Orchestrator = workflow logic, modules = domain logic, TUI = presentation
2. **Fail-Safe Defaults**: Auto mode prefers local â†’ cloud â†’ none (never hard fail)
3. **Data Over Side-Effects**: Return structured dicts, let caller decide how to display
4. **Composability**: Each function does one thing well, can be chained programmatically
5. **OPSEC-First**: Default to local processing, explicit opt-in for cloud
6. **Graceful Degradation**: Missing LLM backend â†’ return partial results, not crash
7. **Testability**: Pure functions with predictable return values

---

## Appendix A: Example Return Schemas

### `analyze_linpeas()` Success
```json
{
  "status": "success",
  "json_path": null,
  "parsed_data": {
    "metadata": {"schema": "don-trabajo-linpeas-v1", "version": "1.0.0"},
    "users": ["root", "www-data"],
    "suid_binaries": ["/usr/bin/sudo", "/usr/bin/passwd"],
    "kernel": {"raw": "5.4.0-42-generic", "version": "5.4.0"},
    "ip_addresses": ["10.10.10.5", "192.168.1.100"],
    "binaries": [{"name": "sudo", "version": "1.8.31"}]
  },
  "cve_findings": [
    {
      "name": "sudo",
      "version": "1.8.31",
      "cve": "CVE-2021-3156",
      "description": "Sudo Baron Samedit heap overflow"
    }
  ],
  "llm_summary": "Target has vulnerable sudo (CVE-2021-3156). Recommend testing Baron Samedit exploit...",
  "errors": []
}
```

### `analyze_linpeas()` Partial (LLM unavailable)
```json
{
  "status": "partial",
  "json_path": null,
  "parsed_data": {...},
  "cve_findings": [...],
  "llm_summary": null,
  "errors": ["LLM backend unavailable (mode=auto, checked local+cloud)"]
}
```

### `get_llm_status()`
```json
{
  "local": {
    "available": true,
    "endpoint": "http://localhost:11434",
    "model": "gpt-oss:20b"
  },
  "cloud": {
    "available": false,
    "api_key_set": false,
    "model": "gpt-3.5-turbo"
  }
}
```

---

## Appendix B: Error Handling Strategy

### Non-Fatal Errors (status="partial")
- LLM backend unavailable when mode="auto"
- CVE matcher finds no results
- Schema validation warnings

**Behavior:** Return partial results with error list, let caller decide if acceptable

### Fatal Errors (status="error")
- Input file not found
- Input file is not valid linPEAS output (preprocessing fails)
- JSON schema validation fails completely
- Explicitly requested backend unavailable (mode="local" but Ollama down)

**Behavior:** Return error status with descriptive message, empty data fields

### Exception Handling Pattern
```python
def analyze_linpeas(...) -> dict:
    try:
        # ... workflow steps ...
        return {"status": "success", ...}
    except FileNotFoundError as e:
        return {"status": "error", "errors": [f"File not found: {e}"]}
    except Exception as e:
        return {"status": "error", "errors": [f"Unexpected error: {e}"]}
```

---

**End of Design Document**

*Ready for Codex implementation. Estimated effort: 4-6 hours for Phase 1-3, 2 hours for Phase 4-5.*
